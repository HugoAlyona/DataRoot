{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from csv file and take part of it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCsv(filename):\n",
    "    # pandas function to generate array fr csv\n",
    "    return  pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loadCsv(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize = 50000\n",
    "indices = np.random.permutation(train.shape[0])\n",
    "    \n",
    "# Split indicies for training and test set by trainSize\n",
    "training_idx = indices[:trainSize]\n",
    "    \n",
    " # Create training and test sets by indicies\n",
    "training = train.loc[train.index.isin(training_idx)]\n",
    "#training.to_csv(\"d\"+str(trainSize)+\".csv\", sep=',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "work with small part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check if the property are different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_column = 'false'\n",
    "#delete_column = 'true' if df['DAY_TYPE'].all() == 'A' else 'false'\n",
    "#print(delete_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_column = 'false'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.columns)\n",
    "#if delete_column:\n",
    "df.drop(['DAY_TYPE'], axis=1, inplace=True)\n",
    "#print(df)\n",
    "\n",
    "\n",
    "df = df[df.POLYLINE != '[]']\n",
    "\n",
    "df = df[df.MISSING_DATA != 1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "convert TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "false\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "def get_year_day(timestamp):\n",
    "    return dt.datetime.utcfromtimestamp(timestamp).strftime(\"%j\")\n",
    "\n",
    "\n",
    "df['YEARDAY'] =  df['TIMESTAMP'].apply(get_year_day)\n",
    "\n",
    "#print(df['TIMESTAMP'])\n",
    "\n",
    "\n",
    "is_the_same_yearday = 'true' if df['YEARDAY'].all() == df['YEARDAY'].any() else 'false'\n",
    "print(is_the_same_yearday)\n",
    "\n",
    "def get_month_day(timestamp):\n",
    "    return dt.datetime.utcfromtimestamp(timestamp).strftime(\"%d\")\n",
    "\n",
    "\n",
    "df['MONTHDAY'] =  df['TIMESTAMP'].apply(get_month_day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "is_the_same_monthday = 'true' if df['MONTHDAY'].all() == df['MONTHDAY'].any() else 'false'\n",
    "print(is_the_same_monthday)\n",
    "\n",
    "\n",
    "\n",
    "def get_week_day(timestamp):\n",
    "    return dt.datetime.utcfromtimestamp(timestamp).strftime(\"%w\")\n",
    "\n",
    "\n",
    "df['WEEKDAY'] =  df['TIMESTAMP'].apply(get_week_day)\n",
    "\n",
    "\n",
    "is_the_same_weekday = 'true' if df['WEEKDAY'].all() == df['WEEKDAY'].any() else 'false'\n",
    "print(is_the_same_weekday)\n",
    "\n",
    "def convert_timestamp(timestamp):\n",
    "    return int(dt.datetime.utcfromtimestamp(timestamp).strftime(\"%H\"))*60.0 + int(dt.datetime.utcfromtimestamp(timestamp).strftime(\"%m\"))\n",
    "\n",
    "df['TIMESTAMP'] = df['TIMESTAMP'].apply(convert_timestamp)\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(value):  \n",
    "    array = np.array([int(18*60), int(8*60+30), int(4*60), int(14*60+30)])\n",
    "    idx = (np.abs(array-(int(value)))).argmin()\n",
    "    return idx\n",
    "\n",
    "df['TIMESTAMP'] = df['TIMESTAMP'].apply(find_nearest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "add total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def total_time(POLYLINE):\n",
    "    return float(len(POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\"))*15)\n",
    "\n",
    "df['TOTAL_TIME'] = df['POLYLINE'].apply(total_time)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "add first lon\n",
    "    fisrt lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_first_lon(POLYLINE):\n",
    "    return float(POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\")[0][1:-1].split(\",\")[0])\n",
    "\n",
    "df['FIRST_LON'] = df['POLYLINE'].apply(add_first_lon)\n",
    "\n",
    "def add_first_lat(POLYLINE):\n",
    "      return float(POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\")[0][1:-1].split(\",\")[1])\n",
    "\n",
    "df['FIRST_LAT'] = df['POLYLINE'].apply(add_first_lat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random trant in polyline\n",
    "add cut lat lat\n",
    "    cut last lon\n",
    "    dif lon\n",
    "    dif lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def get_cut_lon(POLYLINE):\n",
    "    arr = POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\")\n",
    "    lenght = len(arr)\n",
    "    cut = randint(0, lenght-1)\n",
    "    return float(arr[cut][1:-1].split(\",\")[0])\n",
    "  \n",
    "\n",
    "def get_cut_lat(POLYLINE):\n",
    "    arr = POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\")\n",
    "    lenght = len(arr)\n",
    "    cut = randint(0, lenght-1)\n",
    "    return float(arr[cut][1:-1].split(\",\")[1])\n",
    "    \n",
    "\n",
    "df['CUT_LAST_LON'] = df['POLYLINE'].apply(get_cut_lon)\n",
    "\n",
    "df['CUT_LAST_LAT'] = df['POLYLINE'].apply(get_cut_lat)\n",
    "\n",
    "df['DIF_LON']  = df['CUT_LAST_LON'] - df['FIRST_LON']\n",
    "df['DIF_LAT']  = df['CUT_LAST_LAT'] - df['FIRST_LAT']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delete TRIP_ID, MISSING_DATA, POLYLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "df.drop(['MISSING_DATA'], axis=1, inplace=True)\n",
    "df.drop(['POLYLINE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "divide by CALL Type and TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A0 = df[df['CALL_TYPE'] == 'A']\n",
    "df_A0 = df_A0[df_A0['TIMESTAMP'] ==  0]\n",
    "\n",
    "\n",
    "df_A1 = df[df['CALL_TYPE'] == 'A']\n",
    "df_A1 = df_A1[df_A1['TIMESTAMP'] ==  1]\n",
    "\n",
    "\n",
    "df_A2 = df[df['CALL_TYPE'] == 'A']\n",
    "df_A2 = df_A2[df_A2['TIMESTAMP'] ==  2]\n",
    "\n",
    "\n",
    "df_A3 = df[df['CALL_TYPE'] == 'A']\n",
    "df_A3 = df_A3[df_A3['TIMESTAMP'] ==  3]\n",
    "\n",
    "\n",
    "df_B0 = df[df['CALL_TYPE'] == 'B']\n",
    "df_B0 = df_B0[df_B0['TIMESTAMP'] ==  0]\n",
    "\n",
    "\n",
    "df_B1 = df[df['CALL_TYPE'] == 'B']\n",
    "df_B1 = df_B1[df_B1['TIMESTAMP'] ==  1]\n",
    "\n",
    "\n",
    "df_B2 = df[df['CALL_TYPE'] == 'B']\n",
    "df_B2 = df_B2[df_B2['TIMESTAMP'] ==  2]\n",
    "\n",
    "\n",
    "df_B3 = df[df['CALL_TYPE'] == 'B']\n",
    "df_B3 = df_B3[df_B3['TIMESTAMP'] ==  3]\n",
    "\n",
    "\n",
    "\n",
    "df_C0 = df[df['CALL_TYPE'] == 'C']\n",
    "df_C0 = df_C0[df_C0['TIMESTAMP'] ==  0]\n",
    "\n",
    "df_C1 = df[df['CALL_TYPE'] == 'C']\n",
    "df_C1 = df_C1[df_C1['TIMESTAMP'] ==  1]\n",
    "\n",
    "\n",
    "df_C2 = df[df['CALL_TYPE'] == 'C']\n",
    "df_C2 = df_C2[df_C2['TIMESTAMP'] ==  2]\n",
    "\n",
    "\n",
    "df_C3 = df[df['CALL_TYPE'] == 'C']\n",
    "df_C3 = df_C3[df_C3['TIMESTAMP'] ==  3]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delete CALL_TYPE de we have sets for each CALL_TYPE independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A0.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_A1.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_A2.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_A3.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_B0.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_B1.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_B2.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_B3.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_C0.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_C1.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_C2.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "df_C3.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "\n",
    "df_A0.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_A1.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_A2.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_A3.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_C0.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_C1.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_C2.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "df_C3.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "\n",
    "df_B0.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_B1.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_B2.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_B3.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_C0.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_C1.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_C2.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "df_C3.drop(['ORIGIN_CALL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "divote data to x_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99611, 12)\n",
      "(99611,)\n",
      "(110610, 12)\n",
      "(110610,)\n",
      "(54402, 12)\n",
      "(54402,)\n",
      "(99721, 12)\n",
      "(99721,)\n",
      "(234721, 12)\n",
      "(234721,)\n",
      "(227737, 12)\n",
      "(227737,)\n",
      "(117968, 12)\n",
      "(117968,)\n",
      "(235629, 12)\n",
      "(235629,)\n",
      "(125399, 11)\n",
      "(125399,)\n",
      "(98452, 11)\n",
      "(98452,)\n",
      "(193474, 11)\n",
      "(193474,)\n",
      "(107035, 11)\n",
      "(107035,)\n"
     ]
    }
   ],
   "source": [
    "df_A0_y_train = df_A0['TOTAL_TIME']\n",
    "df_A0_x_train = df_A0.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_A0_x_train.shape)\n",
    "print(df_A0_y_train.shape)\n",
    "\n",
    "df_A1_y_train = df_A1['TOTAL_TIME']\n",
    "df_A1_x_train = df_A1.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_A1_x_train.shape)\n",
    "print(df_A1_y_train.shape)\n",
    "\n",
    "df_A2_y_train = df_A2['TOTAL_TIME']\n",
    "df_A2_x_train = df_A2.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_A2_x_train.shape)\n",
    "print(df_A2_y_train.shape)\n",
    "\n",
    "\n",
    "df_A3_y_train = df_A3['TOTAL_TIME']\n",
    "df_A3_x_train = df_A3.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_A3_x_train.shape)\n",
    "print(df_A3_y_train.shape)\n",
    "\n",
    "df_B0_y_train = df_B0['TOTAL_TIME']\n",
    "df_B0_x_train = df_B0.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_B0_x_train.shape)\n",
    "print(df_B0_y_train.shape)\n",
    "\n",
    "df_B1_y_train = df_B1['TOTAL_TIME']\n",
    "df_B1_x_train = df_B1.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_B1_x_train.shape)\n",
    "print(df_B1_y_train.shape)\n",
    "\n",
    "df_B2_y_train = df_B2['TOTAL_TIME']\n",
    "df_B2_x_train = df_B2.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_B2_x_train.shape)\n",
    "print(df_B2_y_train.shape)\n",
    "\n",
    "\n",
    "df_B3_y_train = df_B3['TOTAL_TIME']\n",
    "df_B3_x_train = df_B3.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_B3_x_train.shape)\n",
    "print(df_B3_y_train.shape)\n",
    "\n",
    "\n",
    "df_C0_y_train = df_C0['TOTAL_TIME']\n",
    "df_C0_x_train = df_C0.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_C0_x_train.shape)\n",
    "print(df_C0_y_train.shape)\n",
    "\n",
    "df_C1_y_train = df_C1['TOTAL_TIME']\n",
    "df_C1_x_train = df_C1.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_C1_x_train.shape)\n",
    "print(df_C1_y_train.shape)\n",
    "\n",
    "df_C2_y_train = df_C2['TOTAL_TIME']\n",
    "df_C2_x_train = df_C2.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_C2_x_train.shape)\n",
    "print(df_C2_y_train.shape)\n",
    "\n",
    "\n",
    "df_C3_y_train = df_C3['TOTAL_TIME']\n",
    "df_C3_x_train = df_C3.drop(['TOTAL_TIME'], axis=1)\n",
    "print(df_C3_x_train.shape)\n",
    "print(df_C3_y_train.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "replace NAN in ORIGIN_CALL to 0 for B train data\n",
    "replace NAN in ORIGIN_STAND to 0 for A train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A0_x_train['ORIGIN_CALL'] = df_A0_x_train[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_A1_x_train['ORIGIN_CALL'] = df_A1_x_train[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_A2_x_train['ORIGIN_CALL'] = df_A2_x_train[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_A3_x_train['ORIGIN_CALL'] = df_A3_x_train[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "df_B0_x_train['ORIGIN_STAND'] = df_B0_x_train[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_B1_x_train['ORIGIN_STAND'] = df_B1_x_train[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_B2_x_train['ORIGIN_STAND'] = df_B2_x_train[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_B3_x_train['ORIGIN_STAND'] = df_B3_x_train[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "check if data replaced correctlly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create 12 models,\n",
    "for test data - depending on CALL_TYPE and TIMESTAMP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RandomForestRegressor or GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opolishchuk\\Anaconda3\\envs\\ipykernel_py3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\opolishchuk\\Anaconda3\\envs\\ipykernel_py3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1,\n",
       "           oob_score=False, random_state=5, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.log(y_true + 1) - np.log(y_pred + 1))**2))\n",
    "\n",
    "rmsle_score = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "\n",
    "param_grid3 = {\"n_estimators\": [5, 40, 60, 100, 200],\n",
    "              \"n_jobs\": [-1, 4, 8],\n",
    "              \"random_state\" : [5, 15, 23]}\n",
    "    \n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RA0 = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=23)\n",
    "#clf_RA0 = GridSearchCV(clf_RA0, param_grid=param_grid3)\n",
    "#clf_RA0.fit(df_A0_x_train,  df_A0_y_train)\n",
    "#print(clf_RA0.best_params_)\n",
    "#print('clf_RA0')\n",
    "#print(clf_RA0)\n",
    "#rf_scores = cross_val_score(clf_RA0, df_A0_x_train, df_A0_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RA0.fit(df_A0_x_train, df_A0_y_train)\n",
    "#print(\"r a0 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "#print(0.65)\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RA1 = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=5)\n",
    "#clf_RA1 = GridSearchCV(clf_RA1, param_grid=param_grid3)\n",
    "#clf_RA1.fit(df_A1_x_train,  df_A1_y_train)\n",
    "#print(clf_RA1.best_params_)\n",
    "#print('clf_RA1')\n",
    "#rf_scores = cross_val_score(clf_RA1, df_A1_x_train, df_A1_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RA1.fit(df_A1_x_train, df_A1_y_train)\n",
    "#print(\" r a1 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RA2 = RandomForestRegressor(n_estimators=5, n_jobs=-1, random_state=15)\n",
    "#clf_RA2 = GridSearchCV(clf_RA2, param_grid=param_grid3)\n",
    "#clf_RA2.fit(df_A2_x_train,  df_A2_y_train)\n",
    "#print(clf_RA2.best_params_)\n",
    "#print('clf_RA2')\n",
    "#rf_scores = cross_val_score(clf_RA2, df_A2_x_train, df_A2_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RA2.fit(df_A2_x_train, df_A2_y_train)\n",
    "#print(\" r a2 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RA3 = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=15)\n",
    "#clf_RA3 = GridSearchCV(clf_RA3, param_grid=param_grid3)\n",
    "#clf_RA3.fit(df_A3_x_train,  df_A3_y_train)\n",
    "#print(clf_RA3.best_params_)\n",
    "#print('clf_RA3')\n",
    "#rf_scores = cross_val_score(clf_RA3, df_A3_x_train, df_A3_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RA3.fit(df_A3_x_train, df_A3_y_train)\n",
    "#print(\" r a3 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "    \n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RB0 = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=5)\n",
    "#clf_RB0 = GridSearchCV(clf_RB0, param_grid=param_grid3)\n",
    "#clf_RB0.fit(df_B0_x_train,  df_B0_y_train)\n",
    "#print(clf_RB0.best_params_)\n",
    "#print('clf_RB0')\n",
    "#rf_scores = cross_val_score(clf_RB0, df_B0_x_train, df_B0_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RB0.fit(df_B0_x_train, df_B0_y_train)\n",
    "#print(\" r B0 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RB1 = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=23)\n",
    "\n",
    "#clf_RB1 = GridSearchCV(clf_RB1, param_grid=param_grid3)\n",
    "#clf_RB1.fit(df_B1_x_train,  df_B1_y_train)\n",
    "#print(clf_RB1.best_params_)\n",
    "#print('clf_RB1')\n",
    "#rf_scores = cross_val_score(clf_RB1, df_B1_x_train, df_B1_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RB1.fit(df_B1_x_train, df_B1_y_train)\n",
    "#print(\" r B1 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RB2 = RandomForestRegressor(n_estimators=60, n_jobs=-1, random_state=15)\n",
    "#clf_RB2 = GridSearchCV(clf_RB2, param_grid=param_grid3)\n",
    "#clf_RB2.fit(df_B2_x_train,  df_B2_y_train)\n",
    "#print(clf_RB2.best_params_)\n",
    "#print('clf_RB2')\n",
    "#rf_scores = cross_val_score(clf_RB2, df_B2_x_train, df_B2_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RB2.fit(df_B2_x_train, df_B2_y_train)\n",
    "#print(\" r B2 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RB3 = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=5)\n",
    "#clf_RB3 = GridSearchCV(clf_RB3, param_grid=param_grid3)\n",
    "#clf_RB3.fit(df_B3_x_train,  df_B3_y_train)\n",
    "#print(clf_RB3.best_params_)\n",
    "#print('clf_RB3')\n",
    "#rf_scores = cross_val_score(clf_RB3, df_B3_x_train, df_B3_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RB3.fit(df_B3_x_train, df_B3_y_train)\n",
    "#print(\" r B3 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "    \n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RC0 = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=5)\n",
    "#clf_RC0 = GridSearchCV(clf_RC0, param_grid=param_grid3)\n",
    "#clf_RC0.fit(df_C0_x_train,  df_C0_y_train)\n",
    "#print(clf_RC0.best_params_)\n",
    "#print('clf_RC0')\n",
    "#rf_scores = cross_val_score(clf_RC0, df_C0_x_train, df_C0_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RC0.fit(df_C0_x_train, df_C0_y_train)\n",
    "#print(\" r c0 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RC1 = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=5)\n",
    "#clf_RC1 = GridSearchCV(clf_RC1, param_grid=param_grid3)\n",
    "#clf_RC1.fit(df_C1_x_train,  df_C1_y_train)\n",
    "#print(clf_RC1.best_params_)\n",
    "#print('clf_RC1')\n",
    "#rf_scores = cross_val_score(clf_RC1, df_C1_x_train, df_C1_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RC1.fit(df_C1_x_train, df_C1_y_train)\n",
    "#print(\" r c1 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RC2 = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=5)\n",
    "#clf_RC2 = GridSearchCV(clf_RC2, param_grid=param_grid3)\n",
    "#clf_RC2.fit(df_C2_x_train,  df_C2_y_train)\n",
    "#print(clf_RC2.best_params_)\n",
    "#print('clf_RC2')\n",
    "#rf_scores = cross_val_score(clf_RC2, df_C2_x_train, df_C2_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RC2.fit(df_C2_x_train, df_C2_y_train)\n",
    "#print(\" r c2 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_RC3 = RandomForestRegressor(n_estimators=40, n_jobs=-1, random_state=5)\n",
    "#clf_RC3 = GridSearchCV(clf_RC3, param_grid=param_grid3)\n",
    "#clf_RC3.fit(df_C3_x_train,  df_C3_y_train)\n",
    "#print(clf_RC3.best_params_)\n",
    "#print('clf_RC3')\n",
    "#rf_scores = cross_val_score(clf_RC3, df_C3_x_train, df_C3_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_RC3.fit(df_C3_x_train, df_C3_y_train)\n",
    "#print(\" r c3 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=40, presort='auto', random_state=23,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid3 = {\"n_estimators\": [5, 40, 60, 100, 200],\n",
    "              \"random_state\" : [5, 15, 23],\n",
    "              \"max_depth\" : [2, 3, 4]}\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GA0 = GradientBoostingRegressor(n_estimators=5, max_depth=4, random_state=15)\n",
    "\n",
    "#clf_GA0 = GridSearchCV(clf_GA0, param_grid=param_grid3)\n",
    "#clf_GA0.fit(df_A0_x_train,  df_A0_y_train)\n",
    "#print(clf_GA0.best_params_)\n",
    "#print('clf_GA0')\n",
    "#print(clf_GA0)\n",
    "#rf_scores = cross_val_score(clf_GA0, df_A0_x_train, df_A0_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GA0.fit(df_A0_x_train, df_A0_y_train)\n",
    "#print(\"r a0 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GA1 = GradientBoostingRegressor(n_estimators=5, max_depth=4, random_state=23)\n",
    "\n",
    "#clf_GA1 = GridSearchCV(clf_GA1, param_grid=param_grid3)\n",
    "#clf_GA1.fit(df_A1_x_train,  df_A1_y_train)\n",
    "#print(clf_GA1.best_params_)\n",
    "#print('clf_GA1')\n",
    "#print(clf_GA1)\n",
    "#rf_scores = cross_val_score(clf_GA1, df_A1_x_train, df_A1_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GA1.fit(df_A1_x_train, df_A1_y_train)\n",
    "#print(\"r a1 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GA2 = GradientBoostingRegressor(n_estimators=5, max_depth=4, random_state=5)\n",
    "#clf_GA2 = GridSearchCV(clf_GA2, param_grid=param_grid3)\n",
    "#clf_GA2.fit(df_A2_x_train,  df_A2_y_train)\n",
    "#print(clf_GA2.best_params_)\n",
    "#print('clf_GA2')\n",
    "#print(clf_GA2)\n",
    "#rf_scores = cross_val_score(clf_GA2, df_A2_x_train, df_A2_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GA2.fit(df_A2_x_train, df_A2_y_train)\n",
    "#print(\"r a2 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GA3 = GradientBoostingRegressor(n_estimators=5, max_depth=2, random_state=5)\n",
    "#clf_GA3 = GridSearchCV(clf_GA3, param_grid=param_grid3)\n",
    "#clf_GA3.fit(df_A3_x_train,  df_A3_y_train)\n",
    "#print(clf_GA3.best_params_)\n",
    "#print('clf_GA3')\n",
    "#print(clf_GA3)\n",
    "#rf_scores = cross_val_score(clf_GA3, df_A3_x_train, df_A3_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GA3.fit(df_A3_x_train, df_A3_y_train)\n",
    "#print(\"r a3 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "    \n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GB0 = GradientBoostingRegressor(n_estimators=40, max_depth=2, random_state=15)\n",
    "#clf_GB0 = GridSearchCV(clf_GB0, param_grid=param_grid3)\n",
    "#clf_GB0.fit(df_B0_x_train,  df_B0_y_train)\n",
    "#print(clf_GB0.best_params_)\n",
    "#print('clf_GB0')\n",
    "#print(clf_GB0)\n",
    "#rf_scores = cross_val_score(clf_GB0, df_B0_x_train, df_B0_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GB0.fit(df_B0_x_train, df_B0_y_train)\n",
    "#print(\"r B0 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GB1 = GradientBoostingRegressor(n_estimators=40, max_depth=2, random_state=23)\n",
    "#clf_GB1 = GridSearchCV(clf_GB1, param_grid=param_grid3)\n",
    "#clf_GB1.fit(df_B1_x_train,  df_B1_y_train)\n",
    "#print(clf_GB1.best_params_)\n",
    "#print('clf_GB1')\n",
    "#print(clf_GB1)\n",
    "#rf_scores = cross_val_score(clf_GB1, df_B1_x_train, df_B1_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GB1.fit(df_B1_x_train, df_B1_y_train)\n",
    "#print(\"r B1 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GB2 = GradientBoostingRegressor(n_estimators=5, max_depth=2, random_state=15)\n",
    "#clf_GB2 = GridSearchCV(clf_GA3, param_grid=param_grid3)\n",
    "#clf_GB2.fit(df_B2_x_train,  df_B2_y_train)\n",
    "#print(clf_GB2.best_params_)\n",
    "#print('clf_GB2')\n",
    "#print(clf_GB2)\n",
    "#rf_scores = cross_val_score(clf_GB2, df_B2_x_train, df_B2_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GB2.fit(df_B2_x_train, df_B2_y_train)\n",
    "#print(\"r B2 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GB3 = GradientBoostingRegressor(n_estimators=40, max_depth=2, random_state=23)\n",
    "#clf_GB3 = GridSearchCV(clf_GB3, param_grid=param_grid3)\n",
    "#clf_GB3.fit(df_B3_x_train,  df_B3_y_train)\n",
    "#print(clf_GB3.best_params_)\n",
    "#print('clf_GB3')\n",
    "#print(clf_GB3)\n",
    "#rf_scores = cross_val_score(clf_GB3, df_B3_x_train, df_B3_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GB3.fit(df_B3_x_train, df_B3_y_train)\n",
    "#print(\"r B3 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=40, presort='auto', random_state=15,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GC0 = GradientBoostingRegressor(n_estimators=40, max_depth=2, random_state=15)\n",
    "#clf_GC0 = GridSearchCV(clf_GC0, param_grid=param_grid3)\n",
    "#clf_GC0.fit(df_C0_x_train,  df_C0_y_train)\n",
    "#print(clf_GC0.best_params_)\n",
    "#print('clf_GC0')\n",
    "#print(clf_GC0)\n",
    "#rf_scores = cross_val_score(clf_GC0, df_C0_x_train, df_C0_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GC0.fit(df_C0_x_train, df_C0_y_train)\n",
    "#print(\"r C0 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=40, presort='auto', random_state=5,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GC1 = GradientBoostingRegressor(n_estimators=5, max_depth=2, random_state=15)\n",
    "#clf_GC1 = GridSearchCV(clf_GC1, param_grid=param_grid3)\n",
    "#clf_GC1.fit(df_C1_x_train,  df_C1_y_train)\n",
    "#print(clf_GC1.best_params_)\n",
    "#print('clf_GC1')\n",
    "#print(clf_GC1)\n",
    "#rf_scores = cross_val_score(clf_GC1, df_C1_x_train, df_C1_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GC1.fit(df_C1_x_train, df_C1_y_train)\n",
    "#print(\"r C1 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GC2 = GradientBoostingRegressor(n_estimators=40, max_depth=3, random_state=5)\n",
    "#clf_GC2 = GridSearchCV(clf_GC2, param_grid=param_grid3)\n",
    "#clf_GC2.fit(df_C2_x_train,  df_C2_y_train)\n",
    "#print(clf_GC2.best_params_)\n",
    "#print('clf_GC2')\n",
    "#print(clf_GC2)\n",
    "#rf_scores = cross_val_score(clf_GC2, df_C2_x_train, df_C2_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GC2.fit(df_C2_x_train, df_C2_y_train)\n",
    "#print(\"r C2 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n",
    "\n",
    "# Initialize the famous Random Forest Regressor from scikit-learn\n",
    "clf_GC3 = GradientBoostingRegressor(n_estimators=40, max_depth=4, random_state=5)\n",
    "#clf_GC3 = GridSearchCV(clf_GC3, param_grid=param_grid3)\n",
    "#clf_GC3.fit(df_C3_x_train,  df_C3_y_train)\n",
    "#print(clf_GC3.best_params_)\n",
    "#print('clf_GC3')\n",
    "#print(clf_GC3)\n",
    "#rf_scores = cross_val_score(clf_GC3, df_C3_x_train, df_C3_y_train, cv=10, scoring=rmsle_score)\n",
    "clf_GC3.fit(df_C3_x_train, df_C3_y_train)\n",
    "#print(\"r C3 - np.mean(rf_scores):\")\n",
    "#print(-np.mean(rf_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = loadCsv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID   TIMESTAMP  \\\n",
      "0        T1         B          NaN          15.0  20000542  1408039037   \n",
      "1        T2         B          NaN          57.0  20000108  1408038611   \n",
      "2        T3         B          NaN          15.0  20000370  1408038568   \n",
      "3        T4         B          NaN          53.0  20000492  1408039090   \n",
      "4        T5         B          NaN          18.0  20000621  1408039177   \n",
      "5        T6         A      42612.0           NaN  20000607  1408037146   \n",
      "6        T7         B          NaN          15.0  20000310  1408038846   \n",
      "7        T8         A      31780.0           NaN  20000619  1408038948   \n",
      "8        T9         B          NaN           9.0  20000503  1408038563   \n",
      "9       T10         B          NaN          15.0  20000327  1408038021   \n",
      "10      T11         B          NaN          56.0  20000664  1408038267   \n",
      "11      T12         C          NaN           NaN  20000160  1408038946   \n",
      "12      T13         C          NaN           NaN  20000017  1408039130   \n",
      "13      T14         C          NaN           NaN  20000312  1408036255   \n",
      "14      T15         C          NaN           NaN  20000497  1408038388   \n",
      "15      T16         C          NaN           NaN  20000440  1408037740   \n",
      "16      T17         C          NaN           NaN  20000467  1408038804   \n",
      "17      T18         C          NaN           NaN  20000338  1408038215   \n",
      "18      T19         B          NaN          15.0  20000101  1408038749   \n",
      "19      T20         C          NaN           NaN  20000523  1408036754   \n",
      "20      T21         B          NaN          15.0  20000460  1408039125   \n",
      "21      T22         A      85698.0           NaN  20000199  1408039009   \n",
      "22      T23         A      37007.0           NaN  20000480  1408038924   \n",
      "23      T24         B          NaN          15.0  20000250  1408039145   \n",
      "24      T25         B          NaN          12.0  20000903  1408039057   \n",
      "25      T26         B          NaN          49.0  20000247  1408039029   \n",
      "26      T27         B          NaN          15.0  20000263  1408038859   \n",
      "27      T28         B          NaN          15.0  20000500  1408039165   \n",
      "28      T29         B          NaN          21.0  20000617  1408038439   \n",
      "29      T30         B          NaN          15.0  20000288  1408038605   \n",
      "..      ...       ...          ...           ...       ...         ...   \n",
      "290    T298         B          NaN          16.0  20000158  1419171969   \n",
      "291    T299         C          NaN           NaN  20000693  1419166062   \n",
      "292    T300         A      41052.0           NaN  20000051  1419171407   \n",
      "293    T301         C          NaN           NaN  20000100  1419171112   \n",
      "294    T302         A      34651.0           NaN  20000450  1419171742   \n",
      "295    T303         B          NaN          42.0  20000067  1419172159   \n",
      "296    T304         A      59708.0           NaN  20000612  1419172057   \n",
      "297    T305         B          NaN          53.0  20000320  1419171753   \n",
      "298    T306         C          NaN           NaN  20000047  1419171985   \n",
      "299    T307         C          NaN           NaN  20000081  1419171073   \n",
      "300    T308         C          NaN           NaN  20000542  1419162114   \n",
      "301    T309         C          NaN           NaN  20000109  1419167924   \n",
      "302    T310         A      73071.0           NaN  20000548  1419171991   \n",
      "303    T311         C          NaN           NaN  20000247  1419161986   \n",
      "304    T312         A      36542.0           NaN  20000565  1419171813   \n",
      "305    T313         A      75037.0           NaN  20000383  1419171819   \n",
      "306    T314         A      56743.0           NaN  20000345  1419171903   \n",
      "307    T315         A      37332.0           NaN  20000486  1419171879   \n",
      "308    T316         C          NaN           NaN  20000496  1419171893   \n",
      "309    T317         A      48578.0           NaN  20000436  1419171826   \n",
      "310    T318         B          NaN          22.0  20000325  1419171921   \n",
      "311    T319         A      80148.0           NaN  20000281  1419171095   \n",
      "312    T320         A      66812.0           NaN  20000549  1419164220   \n",
      "313    T321         C          NaN           NaN  20000393  1419168199   \n",
      "314    T322         C          NaN           NaN  20000391  1419171201   \n",
      "315    T323         A      70885.0           NaN  20000430  1419171485   \n",
      "316    T324         B          NaN          53.0  20000020  1419170802   \n",
      "317    T325         C          NaN           NaN  20000207  1419172121   \n",
      "318    T326         A      76232.0           NaN  20000667  1419171980   \n",
      "319    T327         A      31208.0           NaN  20000255  1419171420   \n",
      "\n",
      "     MISSING_DATA                                           POLYLINE  \n",
      "0           False  [[-8.585676,41.148522],[-8.585712,41.148639],[...  \n",
      "1           False  [[-8.610876,41.14557],[-8.610858,41.145579],[-...  \n",
      "2           False  [[-8.585739,41.148558],[-8.58573,41.148828],[-...  \n",
      "3           False  [[-8.613963,41.141169],[-8.614125,41.141124],[...  \n",
      "4           False      [[-8.619903,41.148036],[-8.619894,41.148036]]  \n",
      "5           False  [[-8.630613,41.178249],[-8.630613,41.178249],[...  \n",
      "6           False  [[-8.585622,41.148918],[-8.58564,41.1489],[-8....  \n",
      "7           False  [[-8.582922,41.181057],[-8.582004,41.181813],[...  \n",
      "8           False  [[-8.606529,41.14467],[-8.606673,41.144724],[-...  \n",
      "9           False  [[-8.585658,41.148576],[-8.585703,41.148603],[...  \n",
      "10          False  [[-8.591229,41.162706],[-8.591229,41.162715],[...  \n",
      "11          False  [[-8.585694,41.148603],[-8.585757,41.148684],[...  \n",
      "12          False  [[-8.580105,41.159394],[-8.580231,41.159358],[...  \n",
      "13          False  [[-8.665353,41.18616],[-8.666892,41.188311],[-...  \n",
      "14          False  [[-8.649423,41.154354],[-8.650197,41.154138],[...  \n",
      "15          False  [[-8.618373,41.136003],[-8.618382,41.135877],[...  \n",
      "16          False  [[-8.627094,41.166936],[-8.627049,41.1669],[-8...  \n",
      "17          False  [[-8.607861,41.146236],[-8.607915,41.146236],[...  \n",
      "18          False  [[-8.585667,41.148567],[-8.585694,41.148639],[...  \n",
      "19          False  [[-8.667414,41.161671],[-8.667423,41.161662],[...  \n",
      "20          False  [[-8.585712,41.14854],[-8.585703,41.148549],[-...  \n",
      "21          False  [[-8.612757,41.137722],[-8.612658,41.137785],[...  \n",
      "22          False  [[-8.625816,41.151294],[-8.625942,41.15133],[-...  \n",
      "23          False  [[-8.585658,41.148486],[-8.585658,41.148486],[...  \n",
      "24          False  [[-8.630667,41.154867],[-8.630928,41.15466],[-...  \n",
      "25          False  [[-8.60283,41.179644],[-8.603163,41.179716],[-...  \n",
      "26          False  [[-8.59212,41.160735],[-8.591499,41.161689],[-...  \n",
      "27          False  [[-8.585685,41.1489],[-8.586018,41.14899],[-8....  \n",
      "28          False  [[-8.628777,41.160978],[-8.628741,41.160969],[...  \n",
      "29          False  [[-8.58573,41.148513],[-8.585748,41.148846],[-...  \n",
      "..            ...                                                ...  \n",
      "290         False  [[-8.627697,41.175432],[-8.627463,41.174766],[...  \n",
      "291         False  [[-8.661942,41.183703],[-8.661942,41.183694],[...  \n",
      "292         False  [[-8.573454,41.147199],[-8.573499,41.147262],[...  \n",
      "293         False  [[-8.606403,41.144454],[-8.60643,41.144544],[-...  \n",
      "294         False  [[-8.610264,41.140845],[-8.610066,41.140809],[...  \n",
      "295         False  [[-8.612136,41.172849],[-8.612667,41.172804],[...  \n",
      "296         False  [[-8.641251,41.165658],[-8.64126,41.165712],[-...  \n",
      "297         False  [[-8.613891,41.141286],[-8.61435,41.141115],[-...  \n",
      "298         False  [[-8.620461,41.156514],[-8.620506,41.156469],[...  \n",
      "299         False  [[-8.617581,41.14611],[-8.61759,41.146092],[-8...  \n",
      "300         False  [[-8.588457,41.148558],[-8.588844,41.147199],[...  \n",
      "301         False  [[-8.589096,41.155506],[-8.589105,41.155515],[...  \n",
      "302         False  [[-8.668575,41.157891],[-8.668575,41.157909],[...  \n",
      "303         False  [[-8.607456,41.14872],[-8.60706,41.148621],[-8...  \n",
      "304         False  [[-8.617113,41.160042],[-8.617122,41.160069],[...  \n",
      "305         False  [[-8.628984,41.165874],[-8.629074,41.165874],[...  \n",
      "306         False  [[-8.619723,41.161905],[-8.619768,41.161959],[...  \n",
      "307         False  [[-8.623566,41.14431],[-8.623593,41.144328],[-...  \n",
      "308         False  [[-8.610723,41.144472],[-8.610489,41.143743],[...  \n",
      "309         False  [[-8.640603,41.154948],[-8.640045,41.154714],[...  \n",
      "310         False  [[-8.689293,41.168169],[-8.689302,41.16816],[-...  \n",
      "311         False  [[-8.606358,41.144526],[-8.606358,41.144589],[...  \n",
      "312         False  [[-8.612532,41.159493],[-8.612532,41.15952],[-...  \n",
      "313         False  [[-8.667468,41.238099],[-8.667351,41.238288],[...  \n",
      "314         False  [[-8.606466,41.144697],[-8.606484,41.144697],[...  \n",
      "315         False  [[-8.570196,41.159484],[-8.570187,41.158962],[...  \n",
      "316         False  [[-8.613873,41.141232],[-8.613882,41.141241],[...  \n",
      "317         False  [[-8.6481,41.152536],[-8.647461,41.15241],[-8....  \n",
      "318         False  [[-8.571699,41.156073],[-8.570583,41.155929],[...  \n",
      "319         False  [[-8.574561,41.180184],[-8.572248,41.17995],[-...  \n",
      "\n",
      "[320 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "test.drop(['DAY_TYPE'], axis=1, inplace=True)\n",
    "#print(df)\n",
    "\n",
    "\n",
    "test = test[test.POLYLINE != '[]']\n",
    "\n",
    "test = test[test.MISSING_DATA != 1]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['YEARDAY'] =  test['TIMESTAMP'].apply(get_year_day)\n",
    "\n",
    "test['MONTHDAY'] =  test['TIMESTAMP'].apply(get_month_day)\n",
    "\n",
    "test['WEEKDAY'] =  test['TIMESTAMP'].apply(get_week_day)\n",
    "\n",
    "test['TIMESTAMP'] = test['TIMESTAMP'].apply(convert_timestamp)\n",
    "\n",
    "test['TIMESTAMP'] = test['TIMESTAMP'].apply(find_nearest)\n",
    "\n",
    "test['FIRST_LON'] = test['POLYLINE'].apply(add_first_lon)\n",
    "\n",
    "test['FIRST_LAT'] = test['POLYLINE'].apply(add_first_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_cut_lon(POLYLINE):\n",
    "    arr = POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\")\n",
    "    lenght = len(arr)\n",
    "    return float(arr[lenght-1][1:-1].split(\",\")[0])\n",
    "  \n",
    "\n",
    "def get_test_cut_lat(POLYLINE):\n",
    "    arr = POLYLINE[1:-1].replace(\"],[\", \"]_[\").split(\"_\")\n",
    "    lenght = len(arr)\n",
    "    return float(arr[lenght-1][1:-1].split(\",\")[1])\n",
    "    \n",
    "\n",
    "test['CUT_LAST_LON'] = test['POLYLINE'].apply(get_test_cut_lon)\n",
    "\n",
    "test['CUT_LAST_LAT'] = test['POLYLINE'].apply(get_test_cut_lat)\n",
    "\n",
    "test['DIF_LON']  = test['CUT_LAST_LON'] - test['FIRST_LON']\n",
    "test['DIF_LAT']  = test['CUT_LAST_LAT'] - test['FIRST_LAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test.drop(['MISSING_DATA'], axis=1, inplace=True)\n",
    "test.drop(['POLYLINE'], axis=1, inplace=True)\n",
    "origin_index = test['TRIP_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  TIMESTAMP YEARDAY  \\\n",
      "228    T236         B          NaN          57.0  20000252          2     305   \n",
      "230    T238         B          NaN          57.0  20000554          2     305   \n",
      "239    T247         B          NaN          13.0  20000603          2     305   \n",
      "242    T250         B          NaN          58.0  20000492          2     305   \n",
      "246    T254         B          NaN          57.0  20000285          2     305   \n",
      "249    T257         B          NaN          23.0  20000406          2     305   \n",
      "250    T258         B          NaN          10.0  20000239          2     305   \n",
      "254    T262         B          NaN          23.0  20000361          2     305   \n",
      "265    T273         B          NaN          23.0  20000431          2     305   \n",
      "273    T281         B          NaN          38.0  20000476          2     305   \n",
      "275    T283         B          NaN          14.0  20000577          2     305   \n",
      "276    T284         B          NaN          23.0  20000517          2     305   \n",
      "283    T291         B          NaN          23.0  20000513          2     305   \n",
      "\n",
      "    MONTHDAY WEEKDAY  FIRST_LON  FIRST_LAT  CUT_LAST_LON  CUT_LAST_LAT  \\\n",
      "228       01       6  -8.610750  41.145714     -8.609499     41.146767   \n",
      "230       01       6  -8.610777  41.145597     -8.585766     41.120460   \n",
      "239       01       6  -8.628489  41.157522     -8.687502     41.171580   \n",
      "242       01       6  -8.604828  41.161185     -8.579619     41.165793   \n",
      "246       01       6  -8.610831  41.145687     -8.608707     41.147433   \n",
      "249       01       6  -8.612820  41.145957     -8.627346     41.152437   \n",
      "250       01       6  -8.607132  41.150304     -8.607717     41.148999   \n",
      "254       01       6  -8.612712  41.145975     -8.609319     41.180211   \n",
      "265       01       6  -8.612613  41.145966     -8.610084     41.153895   \n",
      "273       01       6  -8.604126  41.161221     -8.611029     41.148081   \n",
      "275       01       6  -8.610921  41.149440     -8.602137     41.147181   \n",
      "276       01       6  -8.612559  41.145948     -8.610345     41.150439   \n",
      "283       01       6  -8.612802  41.145930     -8.607996     41.159988   \n",
      "\n",
      "      DIF_LON   DIF_LAT  \n",
      "228  0.001251  0.001053  \n",
      "230  0.025011 -0.025137  \n",
      "239 -0.059013  0.014058  \n",
      "242  0.025209  0.004608  \n",
      "246  0.002124  0.001746  \n",
      "249 -0.014526  0.006480  \n",
      "250 -0.000585 -0.001305  \n",
      "254  0.003393  0.034236  \n",
      "265  0.002529  0.007929  \n",
      "273 -0.006903 -0.013140  \n",
      "275  0.008784 -0.002259  \n",
      "276  0.002214  0.004491  \n",
      "283  0.004806  0.014058  \n"
     ]
    }
   ],
   "source": [
    "test_a0 = test[test['CALL_TYPE'] == 'A']\n",
    "test_a0 = test_a0[test_a0['TIMESTAMP'] ==  0]\n",
    "\n",
    "test_a1 = test[test['CALL_TYPE'] == 'A']\n",
    "test_a1 = test_a1[test_a1['TIMESTAMP'] ==  1]\n",
    "\n",
    "test_a2 = test[test['CALL_TYPE'] == 'A']\n",
    "test_a2 = test_a2[test_a2['TIMESTAMP'] ==  2]\n",
    "\n",
    "test_a3 = test[test['CALL_TYPE'] == 'A']\n",
    "test_a3 = test_a3[test_a3['TIMESTAMP'] ==  3]\n",
    "\n",
    "test_b0 = test[test['CALL_TYPE'] == 'B']\n",
    "test_b0 = test_b0[test_b0['TIMESTAMP'] ==  0]\n",
    "\n",
    "test_b1 = test[test['CALL_TYPE'] == 'B']\n",
    "test_b1 = test_b1[test_b1['TIMESTAMP'] ==  1]\n",
    "\n",
    "test_b2 = test[test['CALL_TYPE'] == 'B']\n",
    "test_b2 = test_b2[test_b2['TIMESTAMP'] ==  2]\n",
    "print(test_b2)\n",
    "\n",
    "test_b3 = test[test['CALL_TYPE'] == 'B']\n",
    "test_b3 = test_b3[test_b3['TIMESTAMP'] ==  3]\n",
    "\n",
    "\n",
    "test_c0 = test[test['CALL_TYPE'] == 'C']\n",
    "test_c0 = test_c0[test_c0['TIMESTAMP'] ==  0]\n",
    "\n",
    "test_c1 = test[test['CALL_TYPE'] == 'C']\n",
    "test_c1 = test_c1[test_c1['TIMESTAMP'] ==  1]\n",
    "\n",
    "test_c2 = test[test['CALL_TYPE'] == 'C']\n",
    "test_c2 = test_c2[test_c2['TIMESTAMP'] ==  2]\n",
    "\n",
    "test_c3 = test[test['CALL_TYPE'] == 'C']\n",
    "test_c3 = test_c3[test_c3['TIMESTAMP'] ==  3]\n",
    "\n",
    "test_a0['ORIGIN_CALL'] = test_a0[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "test_a1['ORIGIN_CALL'] = test_a1[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "test_a2['ORIGIN_CALL'] = test_a2[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "test_a3['ORIGIN_CALL'] = test_a3[\"ORIGIN_CALL\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "test_b0['ORIGIN_STAND'] = test_b0[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "test_b1['ORIGIN_STAND'] = test_b1[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "test_b2['ORIGIN_STAND'] = test_b2[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "test_b3['ORIGIN_STAND'] = test_b3[\"ORIGIN_STAND\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "\n",
    "test_a0.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_a1.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_a2.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_a3.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_b0.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_b1.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_b2.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_b3.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_c0.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_c1.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_c2.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "test_c3.drop(['CALL_TYPE'], axis=1, inplace=True)\n",
    "\n",
    "test_a0.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_a1.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_a2.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_a3.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_c0.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_c1.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_c2.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "test_c3.drop(['ORIGIN_STAND'], axis=1, inplace=True)\n",
    "\n",
    "test_b0.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_b1.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_b2.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_b3.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_c0.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_c1.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_c2.drop(['ORIGIN_CALL'], axis=1, inplace=True)\n",
    "test_c3.drop(['ORIGIN_CALL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a0_x=test_a0.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_a0 = clf_RA0.predict(test_a0_x)\n",
    "y_pred_gf_a0 = clf_GA0.predict(test_a0_x)\n",
    "test_a0[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_a0 + y_pred_gf_a0)\n",
    "\n",
    "test_a1_x=test_a1.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_a1 = clf_RA1.predict(test_a1_x)\n",
    "y_pred_gf_a1 = clf_GA1.predict(test_a1_x)\n",
    "test_a1[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_a1 + y_pred_gf_a1)\n",
    "\n",
    "test_a2_x=test_a2.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_a2 = clf_RA2.predict(test_a2_x)\n",
    "y_pred_gf_a2 = clf_GA2.predict(test_a2_x)\n",
    "test_a2[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_a2 + y_pred_gf_a2)\n",
    "\n",
    "test_a3_x=test_a3.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_a3 = clf_RA3.predict(test_a3_x)\n",
    "y_pred_gf_a3 = clf_GA3.predict(test_a3_x)\n",
    "test_a3[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_a3 + y_pred_gf_a3)\n",
    "\n",
    "test_b0_x=test_b0.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_b0 = clf_RB0.predict(test_b0_x)\n",
    "y_pred_gf_b0 = clf_GB0.predict(test_b0_x)\n",
    "test_b0[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_b0 + y_pred_gf_b0)\n",
    "\n",
    "test_b1_x=test_b1.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_b1 = clf_RB1.predict(test_b1_x)\n",
    "y_pred_gf_b1 = clf_GB1.predict(test_b1_x)\n",
    "test_b1[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_b1 + y_pred_gf_b1)\n",
    "\n",
    "test_b2_x=test_b2.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_b2 = clf_RB2.predict(test_b2_x)\n",
    "y_pred_gf_b2 = clf_GB2.predict(test_b2_x)\n",
    "test_b2[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_b2 + y_pred_gf_b2)\n",
    "\n",
    "test_b3_x=test_b3.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_b3 = clf_RB3.predict(test_b3_x)\n",
    "y_pred_gf_b3 = clf_GB3.predict(test_b3_x)\n",
    "test_b3[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_b3 + y_pred_gf_b3)\n",
    "\n",
    "\n",
    "test_c0_x=test_c0.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_c0 = clf_RC0.predict(test_c0_x)\n",
    "y_pred_gf_c0 = clf_GC0.predict(test_c0_x)\n",
    "test_c0[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_c0 + y_pred_gf_c0)\n",
    "\n",
    "test_c1_x=test_c1.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_c1 = clf_RC1.predict(test_c1_x)\n",
    "y_pred_gf_c1 = clf_GC1.predict(test_c1_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_c1[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_c1 + y_pred_gf_c1)\n",
    "\n",
    "test_c2_x=test_c2.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_c2 = clf_RC2.predict(test_c2_x)\n",
    "y_pred_gf_c2 = clf_GC2.predict(test_c2_x)\n",
    "test_c2[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_c2 + y_pred_gf_c2)\n",
    "\n",
    "test_c3_x=test_c3.drop(['TRIP_ID'], axis=1)\n",
    "y_pred_rf_c3 = clf_RC3.predict(test_c3_x)\n",
    "y_pred_gf_c3 = clf_GC3.predict(test_c3_x)\n",
    "test_c3[\"TRAVEL_TIME\"] = 0.5*(y_pred_rf_c3 + y_pred_gf_c3)\n",
    "\n",
    "\n",
    "\n",
    "#test_a0[[\"TRIP_ID\",\"TRAVEL_TIME\"]].to_csv(\"res0004.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = test_a0.append(test_a1).append(test_a2).append(test_a3).append(test_b0).append(test_b1).append(test_b2).append(test_b3).append(test_c0).append(test_c1).append(test_c2).append(test_c3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(line):\n",
    "    return \"\" + str(line)\n",
    "res.sort_index(inplace=True)\n",
    "print(res)\n",
    "res[[\"TRIP_ID\",\"TRAVEL_TIME\"]].to_csv(\"res0104_mix.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-c1a47e6afb41>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-85-c1a47e6afb41>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    origin_index_a2 = pd.concat(([origin_index_a2, y_pred_rf_a2],axis=1)\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "origin_index_a2 = test_a2['TRIP_ID']\n",
    "test_a2.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_a2 = clf_RA2.predict(test_a2)\n",
    "origin_index_a2 = pd.concat(([origin_index_a2, y_pred_rf_a2],axis=1)\n",
    "\n",
    "origin_index_a3 = test_a3['TRIP_ID']\n",
    "test_a3.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_a3 = clf_RA3.predict(test_a3)\n",
    "origin_index_a3 = pd.concat(([origin_index_a3, y_pred_rf_a3],axis=1)\n",
    "\n",
    "origin_index_b0 = test_b0['TRIP_ID']\n",
    "test_b0.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_b0 = clf_RB0.predict(test_b0)\n",
    "origin_index_b0 = pd.concat(([origin_index_b0, y_pred_rf_b0],axis=1)\n",
    "\n",
    "origin_index_b1 = test_b1['TRIP_ID']\n",
    "test_b1.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_b1 = clf_RB1.predict(test_b1)\n",
    "origin_index_b1 = pd.concat(([origin_index_b1, y_pred_rf_b1],axis=1)\n",
    "\n",
    "origin_index_b2 = test_b2['TRIP_ID']\n",
    "test_b2.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_b2 = clf_RB2.predict(test_b2)\n",
    "origin_index_b2 = pd.concat(([origin_index_b2, y_pred_rf_b2],axis=1)\n",
    "\n",
    "origin_index_b3 = test_b3['TRIP_ID']\n",
    "test_b3.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_b3 = clf_RB3.predict(test_b3)\n",
    "origin_index_b3 = pd.concat(([origin_index_b3, y_pred_rf_b3],axis=1)\n",
    "\n",
    "origin_index_c0 = test_c0['TRIP_ID']\n",
    "test_c0.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_c0 = clf_RC0.predict(test_c0)\n",
    "origin_index_c0 = pd.concat(([origin_index_c0, y_pred_rf_c0],axis=1)\n",
    "\n",
    "origin_index_c1 = test_a2['TRIP_ID']\n",
    "test_c1.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_c1 = clf_RC1.predict(test_c1)\n",
    "origin_index_c1 = pd.concat(([origin_index_c1, y_pred_rf_c1],axis=1)\n",
    "\n",
    "origin_index_c2 = test_x_c2['TRIP_ID']\n",
    "test_x_c2.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_c2 = clf_RC2.predict(test_x_c2)\n",
    "origin_index_c2 = pd.concat(([origin_index_c2, y_pred_rf_c2],axis=1)\n",
    "\n",
    "origin_index_c3 = test_c3['TRIP_ID']\n",
    "test_c3.drop(['TRIP_ID'], axis=1, inplace=True)\n",
    "y_pred_rf_c3 = clf_RC3.predict(test_c3)\n",
    "origin_index_c3 = pd.concat(([origin_index_c3, y_pred_rf_c3],axis=1)\n",
    "\n",
    "print(\"origin_index_a0\")\n",
    "print(origin_index_a0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
